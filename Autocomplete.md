Week 3 of Course 2 

Table of Contents:
	[[#N-Grams]]
	[[#Sequence Probabilities]]
	[[#Starting and Ending Sentences]]
	[[#N-gram language Model]]
	[[#Language Model Evaluation]]
		[[#**Train/Val/Test splits**]]
		[[#**Perplexity**]]
	[[#Out of Vocabulary Words]]
	[[#Smoothing]]
	[[#Week Summary]]

[[C2_W3.pdf|Week 3 slides]]

---


# N-Grams

**Applications of N-Grams** - Usage include:
- Speech Recognition
- Spelling correction/[[Autocomplete]]
- Augmentative Communication (predict most likely word from menu for people unable to physically talk or sign)



**N-Grams Definition** - An N-gram is a sequence of N words
	![[Pasted image 20240325141457.png]]
	![[Pasted image 20240325141509.png]]
	![[Pasted image 20240325141534.png]]


---

# Sequence Probabilities

You just saw how to compute sequence probabilities, their short comings, and finally how to approximate N-gram probabilities. In doing so, you try to approximate the probability of a sentence. For example, what is the probability of the following sentence: _The teacher drinks tea._ To compute it, you will make use of the following:
![[Pasted image 20240325142030.png]]


To compute the probability of a sequence, you can compute the following:
![[Pasted image 20240325142057.png]]

One of the main issues with computing the probabilities above is the corpus rarely contains the exact same phrases as the ones you computed your probabilities on. Hence, you can easily end up getting a probability of 0. The _Markov assumption_ indicates that only the last word matters. Hence:
![[Pasted image 20240325142112.png]]

You can model the entire sentence as follows:
![[Pasted image 20240325142132.png]]


---

# Starting and Ending Sentences

We usually start and end a sentence with the following tokens respectively: \<s> \</s>.

When computing probabilities using a unigram, you can append an \<s> in the beginning of the sentence. To generalize to an N-gram language model, you can add N-1 start tokens 
\<s>.

For the end of sentence token \</s>, you only need one even if it is an N-gram. Here is an example:

![[Pasted image 20240325143716.png]]


---

# N-gram language Model

![[Pasted image 20240325144027.png]]
![[Pasted image 20240325144102.png]]

---

# Language Model Evaluation

Splitting the Data
We will now discuss the train/val/test splits and perplexity.

### **Train/Val/Test splits**

Smaller Corpora: 
- 80% train
- 10% val 
- 10% test

Larger Corpora: 
- 98% train
- 1% val 
- 1% test

There are two main methods for splitting the data: 
![[Pasted image 20240325144821.png]]



### **Perplexity**

Perplexity is used to tell us whether a set of sentences look like they were written by humans rather than by a simple program choosing words at random. A text that is written by humans is more likely to have lower perplexity, where a text generated by random word choice would have a higher perplexity.

Concretely, here are the formulas to calculate perplexity. 
![[Pasted image 20240325144859.png]]


---


# Out of Vocabulary Words

Many times, you will be dealing with unknown words in the corpus. So how do you choose your vocabulary? What is a vocabulary?

A vocabulary is a set of unique words supported by your language model. In some tasks like speech recognition or question answering, you will encounter and generate words only from a fixed set of words. Hence, a **closed vocabulary**.

**Open vocabulary** means that you may encounter words from outside the vocabulary, like a name of a new city in the training set. Here is one recipe that would allow you to handle unknown words.

- Create vocabulary V
- Replace any word in corpus and not in V by \<UNK>
- Count the probabilities with \<UNK> as with any other word

![[Pasted image 20240325145230.png]]

The example above shows how you can use min_frequency and replace all the words that show up fewer times than min_frequency by UNK. You can then treat UNK as a regular word. 

**Criteria to create the vocabulary**
 - Min word frequency f
 - Max |V|, include words by frequency
 - Use \<UNK> sparingly (Why?)
 - **Perplexity** -  only compare LMs with the same V

---

# Smoothing

The three main concepts covered here are dealing with missing n-grams, smoothing, and Backoff and interpolation. 

![[Pasted image 20240325145445.png]]


When using back-off:

- If N-gram missing => use (N-1)-gram, …: Using the lower level N-grams (i.e. (N-1)-gram, (N-2)-gram, down to unigram) distorts the probability distribution. Especially for smaller corpora, some probability needs to be discounted from higher level N-grams to use it for lower level N-grams.
- Probability discounting e.g. Katz backoff: makes use of discounting. 
- “Stupid” backoff: If the higher order N-gram probability is missing, the lower order N-gram probability is used, just multiplied by a constant. A constant of about 0.4 was experimentally shown to work well.

Here is a visualization: 
![[Pasted image 20240325145520.png]]

You can also use interpolation when computing probabilities as follows:
![[Pasted image 20240325145535.png]]

---

# Week Summary

This week you learned the following concepts
- N-Grams and probabilities
- Approximate sentence probability from N-Grams
- Build a language model from a corpus
- Fix missing information
- Out of vocabulary words with \<UNK>
- Missing N-Gram in corpus with smoothing, backoff and interpolation
- Evaluate language model with perplexity
- Coding assignment! 

